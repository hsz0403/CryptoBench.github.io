<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CryptoBench — A Dynamic Benchmark for Expert-Level LLM Agents</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="assets/css/style.css" />
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <script defer src="assets/js/render.js"></script>
</head>
<body>
    <div class="bg-grid"></div>
    <div class="bg-gradient"></div>
    <div class="bg-noise"></div>
    <header class="navbar">
        <div class="nav-content">
            <a class="logo" href="#top">CryptoBench</a>
            <ul class="nav-links">
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#system">System</a></li>
                <li><a href="#design">Benchmark</a></li>
                <li><a href="#domains">Domains</a></li>
                <li><a href="#pipeline">Methodology</a></li>
                <li><a href="#leaderboard">Leaderboard</a></li>
            </ul>
        </div>
    </header>

    <section class="hero" id="top">
        <div class="hero-content">
            <h1>CryptoBench: Evaluating LLM Agents in the most adversarial market.</h1>
            <p class="subtitle">
                CryptoBench is the first crypto-native benchmark authored by DeFi analysts, on-chain investigators, and derivatives traders. It ships 50 new questions every month, forcing agents to retrieve, reason, and predict under real market pressure.
            </p>
            <div class="hero-buttons">
                <a class="btn btn-primary" href="#" aria-disabled="true">Paper PDF (soon)</a>
                <a class="btn btn-ghost" href="https://github.com/hsz0403/CryptoBench.github.io" target="_blank" rel="noopener">GitHub</a>
                <a class="btn btn-secondary" href="#system">Explore benchmark</a>
                <a class="btn btn-ghost" href="#leaderboard">Leaderboard</a>
            </div>
            <div class="hero-stats">
                <div>
                    <span>50</span>
                    Monthly tasks
                </div>
                <div>
                    <span>4</span>
                    Retrieval × prediction quadrants
                </div>
                <div>
                    <span>20+</span>
                    Live data platforms
                </div>
                <div>
                    <span>10</span>
                    LLM / agent configurations
                </div>
            </div>
        </div>
    </section>

    <main class="main-content">
        <section class="section" id="abstract">
            <div class="section-header">
                <h2>Abstract</h2>
                <p class="section-subtitle">From the CryptoBench paper: “CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency”.</p>
            </div>
            <div class="abstract">
                <p>
                    The paper introduces CryptoBench, a live benchmark that stress-tests LLM agents in time-sensitive, adversarial crypto workflows. Existing agent benchmarks overlook the need to synthesize on-chain intelligence, market data, DEX flows, and MEV alerts. CryptoBench delivers 50 domain-authentic questions per month, categorized into Simple/Complex Retrieval and Simple/Complex Prediction, mirroring professional analyst workloads.
                </p>
                <p>
                    Evaluating ten state-of-the-art LLMs (with and without the SmolAgent framework) reveals a pronounced <em>retrieval–prediction imbalance</em>: models that excel at factual lookup frequently collapse on predictive reasoning. Agentic orchestration can reshuffle leaderboard positions, proving that raw model IQ does not equal field performance.
                </p>
            </div>
        </section>

        <section class="section" id="system">
            <div class="section-header">
                <h2>System Overview</h2>
            </div>
            <div class="figure-stack">
                <article class="figure-full portrait">
                    <h3>Four-Quadrant Framework</h3>
                    <p>Illustrates the Simple/Complex × Retrieval/Prediction matrix that guides task authoring and score reporting each month.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/Figure1.pdf" aria-label="Four-Quadrant Framework"></canvas>
                </article>
                <article class="figure-full portrait">
                    <h3>Benchmark Pipeline</h3>
                    <p>Shows the end-to-end workflow: question authoring, live data capture, agent execution, scoring, and diagnostics.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/Figure2.pdf" aria-label="Benchmark Pipeline"></canvas>
                </article>
            </div>
        </section>

        <section class="section" id="design">
            <div class="section-header">
                <h2>Benchmark Design</h2>
                <p class="section-subtitle">Section 3 of the paper highlights three design principles.</p>
            </div>
            <div class="card-grid">
                <article class="card">
                    <h3>Professional task authorship</h3>
                    <p>Every prompt is drafted by crypto-native researchers—DeFi risk teams, on-chain sleuths, and quantitative traders—to ensure ecological validity.</p>
                </article>
                <article class="card">
                    <h3>Dynamic refresh</h3>
                    <p>Template variables (wallets, tickers, time windows) are swapped quarterly, preventing memorization and keeping pace with new primitives like restaking and intent layers.</p>
                </article>
                <article class="card">
                    <h3>Broad platform coverage</h3>
                    <p>Tasks require navigating on-chain explorers, CeFi + DeFi dashboards, derivatives terminals, DEX routing tools, and AI-generated signal feeds.</p>
                </article>
                <article class="card">
                    <h3>Tool orchestration focus</h3>
                    <p>Agents must pick authoritative sources, adapt to UX quirks, and reconcile conflicting feeds before answering—mirroring real analyst workflows.</p>
                </article>
            </div>
        </section>

        <section class="section" id="domains">
            <div class="section-header">
                <h2>Crypto-native Task Domains</h2>
            </div>
            <div class="sectors-grid">
                <article class="sector-item">
                    <h4>On-chain Intelligence</h4>
                    <p>Wallet archetyping, exploit tracing, whale/KOL flow, phishing and “rat trading” detection.</p>
                </article>
                <article class="sector-item">
                    <h4>Market Data</h4>
                    <p>Liquidations, funding rates, open interest, and macro volatility feeds stitched together in real time.</p>
                </article>
                <article class="sector-item">
                    <h4>DeFi & Oracle Analytics</h4>
                    <p>Protocol dashboards for TVL/TVS, oracle drift analysis, and risk surface comparisons.</p>
                </article>
                <article class="sector-item">
                    <h4>DEX & Derivatives</h4>
                    <p>Order-flow toxicity, AMM routing, basis spreads, and delta-hedge stress testing.</p>
                </article>
                <article class="sector-item">
                    <h4>MEV & AI Signals</h4>
                    <p>Intent mempools, bundle profitability, and model-generated trade alerts from crypto-native terminals.</p>
                </article>
            </div>
        </section>

        <section class="section" id="pipeline">
            <div class="section-header">
                <h2>Dataset Construction & Evaluation Protocol</h2>
                <p class="section-subtitle">Summarized from Sections 3.2 and 4.</p>
            </div>
            <div class="timeline">
                <article>
                    <h3>1 · Prompt templating</h3>
                    <p>Authors lock key variables (addresses, hashes, timeframes) inside reusable templates to accelerate monthly refreshes.</p>
                </article>
                <article>
                    <h3>2 · Evidence capture</h3>
                    <p>Reference answers are harvested from official dashboards with timestamps and stored screenshots for auditing.</p>
                </article>
                <article>
                    <h3>3 · Dual evaluation</h3>
                    <p>Each task is run both as direct LLM calls and through the SmolAgent framework with identical tool permissions.</p>
                </article>
                <article>
                    <h3>4 · Scoring & QC</h3>
                    <p>Outputs are graded for factuality, synthesis, and prediction accuracy; two-person review resolves disputes and logs failure modes.</p>
                </article>
            </div>
            <div class="experiment-setup">
                <p><strong>Models:</strong> Grok-4 (Web & Fast), GPT-5, GPT-OSS-120B, Qwen3-Max, Gemini 2.5 Pro, DeepSeek R1, and others.</p>
                <p><strong>Metrics:</strong> Accuracy per quadrant, macro-category radar, difficulty split, and qualitative diagnostics.</p>
                <p><strong>Time window:</strong> 12–18 Oct, ensuring identical market conditions for every run.</p>
            </div>
        </section>

        <section class="section" id="metrics">
            <div class="section-header">
                <h2>Benchmark Metrics</h2>
                <p class="section-subtitle">Distribution and coverage visualizations that describe how CryptoBench samples tasks and platforms.</p>
            </div>
            <div class="figure-stack">
                <article class="figure-full">
                    <h3>Task Distribution</h3>
                    <p>CryptoBench balances the four quadrants so no single capability can game the benchmark or inflate scores.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/A_task_distribution.pdf" aria-label="Task Distribution"></canvas>
                </article>
                <article class="figure-full">
                    <h3>Platform Distribution</h3>
                    <p>Agents must interact with 20+ live surfaces—from on-chain explorers to intent-based MEV dashboards—to answer correctly.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/B_platform_distribution.pdf" aria-label="Platform Distribution"></canvas>
                </article>
                <article class="figure-full">
                    <h3>Macro-category Radar</h3>
                    <p>Top models perform best in general “Other” tasks, while DeFi analytics, DEX data, and derivatives remain challenging.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/E_performance_by_category_radar.pdf" aria-label="Macro-category Radar"></canvas>
                </article>
            </div>
        </section>

        <section class="section" id="leaderboard">
            <div class="section-header">
                <h2>Leaderboards</h2>
                <p class="section-subtitle">Scorecards for direct LLM calls and SmolAgent runs, plus the most diagnostic breakdowns.</p>
            </div>
            <div class="figure-stack">
                <article class="figure-full">
                    <h3>Combined Evaluation Scores</h3>
                    <p>This overview contrasts direct LLM evaluation with the SmolAgent setup between 12–18 Oct, showing how agentic scaffolding reshuffles rankings.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/3_combined_eval_scores.pdf" aria-label="Combined Evaluation Scores"></canvas>
                </article>
                <article class="figure-full">
                    <h3>LLM-only Leaderboard</h3>
                    <p>Direct prompts reveal Grok-4 (Web) clearly leading at 1.32 points, with GPT-5 trailing at 0.90 and a long tail of underperforming models.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/1_llm_eval_scores.pdf" aria-label="LLM-only Leaderboard"></canvas>
                </article>
                <article class="figure-full">
                    <h3>SmolAgent Leaderboard</h3>
                    <p>When the same models run inside SmolAgent, Grok-4 still leads but Qwen3-Max and Gemini 2.5 Pro close the gap—evidence that planning and tool use matter.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/2_smolagent_eval_scores.pdf" aria-label="SmolAgent Leaderboard"></canvas>
                </article>
                <article class="figure-full">
                    <h3>Performance by Difficulty</h3>
                    <p>All models degrade on complex tasks; Grok-4 drops roughly 10 percentage points while GPT-5 collapses by more than 25.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/C_performance_by_difficulty.pdf" aria-label="Performance by Difficulty"></canvas>
                </article>
                <article class="figure-full">
                    <h3>Retrieval vs. Prediction</h3>
                    <p>The retrieval–prediction imbalance is stark: GPT-5 reaches 41% on retrieval but only 6.25% on prediction.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/D_performance_by_task_family.pdf" aria-label="Retrieval vs. Prediction"></canvas>
                </article>
                <article class="figure-full">
                    <h3>Task Quadrants</h3>
                    <p>Simple retrieval is saturated, but complex prediction exposes the largest gaps, confirming the need for multi-hop reasoning.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/F_performance_by_task_quadrant.pdf" aria-label="Task Quadrants"></canvas>
                </article>
                <article class="figure-full">
                    <h3>Investor Focus Breakdown</h3>
                    <p>Segments tasks by whether retail participants or institutional desks care more about the question, highlighting coverage across investor profiles.</p>
                    <canvas class="figure-canvas" data-pdf="assets/pdfs/G_performance_by_investor_focus.pdf" aria-label="Investor Focus Breakdown"></canvas>
                </article>
            </div>
        </section>
    </main>

    <footer>
        <p>© CryptoBench. Dynamic benchmark for expert-level evaluation of LLM agents in cryptocurrency.</p>
    </footer>
</body>
</html>
