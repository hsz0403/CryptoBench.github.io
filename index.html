<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CryptoBench — A Dynamic Benchmark for Expert-Level LLM Agents</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
    <div class="bg-grid"></div>
    <div class="bg-gradient"></div>
    <div class="bg-noise"></div>
    <header class="navbar">
        <div class="nav-content">
            <a class="logo" href="#top">CryptoBench</a>
            <ul class="nav-links">
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#design">Benchmark</a></li>
                <li><a href="#domains">Domains</a></li>
                <li><a href="#pipeline">Methodology</a></li>
                <li><a href="#figures">Figures</a></li>
                <li><a href="#downloads">Downloads</a></li>
            </ul>
            <div class="nav-actions">
                <a class="btn btn-ghost" href="https://github.com/hsz0403/CryptoBench.github.io" target="_blank" rel="noopener">GitHub</a>
                <a class="btn btn-primary" href="#" aria-disabled="true">Paper PDF (soon)</a>
            </div>
        </div>
    </header>

    <section class="hero" id="top">
        <div class="hero-content">
            <p class="eyebrow">Live window · 12–18 Oct</p>
            <h1>CryptoBench: Evaluating LLM Agents in the most adversarial market.</h1>
            <p class="subtitle">
                CryptoBench is the first crypto-native benchmark authored by DeFi analysts, on-chain investigators, and derivatives traders. It ships 50 new questions every month, forcing agents to retrieve, reason, and predict under real market pressure.
            </p>
            <div class="hero-buttons">
                <a class="btn btn-primary" href="#abstract">Read the paper overview</a>
                <a class="btn btn-secondary" href="#figures">Jump to figures</a>
            </div>
            <div class="hero-stats">
                <div>
                    <span>50</span>
                    Monthly tasks
                </div>
                <div>
                    <span>4</span>
                    Retrieval × prediction quadrants
                </div>
                <div>
                    <span>20+</span>
                    Live data platforms
                </div>
                <div>
                    <span>10</span>
                    LLM / agent configurations
                </div>
            </div>
        </div>
    </section>

    <main class="main-content">
        <section class="section" id="abstract">
            <div class="section-header">
                <h2>Abstract</h2>
                <p class="section-subtitle">From the CryptoBench paper: “CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency”.</p>
            </div>
            <div class="abstract">
                <p>
                    The paper introduces CryptoBench, a live benchmark that stress-tests LLM agents in time-sensitive, adversarial crypto workflows. Existing agent benchmarks overlook the need to synthesize on-chain intelligence, market data, DEX flows, and MEV alerts. CryptoBench delivers 50 domain-authentic questions per month, categorized into Simple/Complex Retrieval and Simple/Complex Prediction, mirroring professional analyst workloads.
                </p>
                <p>
                    Evaluating ten state-of-the-art LLMs (with and without the SmolAgent framework) reveals a pronounced <em>retrieval–prediction imbalance</em>: models that excel at factual lookup frequently collapse on predictive reasoning. Agentic orchestration can reshuffle leaderboard positions, proving that raw model IQ does not equal field performance.
                </p>
            </div>
        </section>

        <section class="section" id="design">
            <div class="section-header">
                <h2>Benchmark Design</h2>
                <p class="section-subtitle">Section 3 of the paper highlights three design principles.</p>
            </div>
            <div class="card-grid">
                <article class="card">
                    <h3>Professional task authorship</h3>
                    <p>Every prompt is drafted by crypto-native researchers—DeFi risk teams, on-chain sleuths, and quantitative traders—to ensure ecological validity.</p>
                </article>
                <article class="card">
                    <h3>Dynamic refresh</h3>
                    <p>Template variables (wallets, tickers, time windows) are swapped quarterly, preventing memorization and keeping pace with new primitives like restaking and intent layers.</p>
                </article>
                <article class="card">
                    <h3>Broad platform coverage</h3>
                    <p>Tasks require navigating on-chain explorers, CeFi + DeFi dashboards, derivatives terminals, DEX routing tools, and AI-generated signal feeds.</p>
                </article>
                <article class="card">
                    <h3>Tool orchestration focus</h3>
                    <p>Agents must pick authoritative sources, adapt to UX quirks, and reconcile conflicting feeds before answering—mirroring real analyst workflows.</p>
                </article>
            </div>
        </section>

        <section class="section" id="domains">
            <div class="section-header">
                <h2>Crypto-native Task Domains</h2>
                <p class="section-subtitle">Section 3.1 enumerates the skill diversity tested each month.</p>
            </div>
            <div class="sectors-grid">
                <article class="sector-item">
                    <h4>On-chain Intelligence</h4>
                    <p>Wallet archetyping, exploit tracing, whale/KOL flow, phishing and “rat trading” detection.</p>
                </article>
                <article class="sector-item">
                    <h4>Market Data</h4>
                    <p>Liquidations, funding rates, open interest, and macro volatility feeds stitched together in real time.</p>
                </article>
                <article class="sector-item">
                    <h4>DeFi & Oracle Analytics</h4>
                    <p>Protocol dashboards for TVL/TVS, oracle drift analysis, and risk surface comparisons.</p>
                </article>
                <article class="sector-item">
                    <h4>DEX & Derivatives</h4>
                    <p>Order-flow toxicity, AMM routing, basis spreads, and delta-hedge stress testing.</p>
                </article>
                <article class="sector-item">
                    <h4>MEV & AI Signals</h4>
                    <p>Intent mempools, bundle profitability, and model-generated trade alerts from crypto-native terminals.</p>
                </article>
            </div>
        </section>

        <section class="section" id="pipeline">
            <div class="section-header">
                <h2>Dataset Construction & Evaluation Protocol</h2>
                <p class="section-subtitle">Summarized from Sections 3.2 and 4.</p>
            </div>
            <div class="timeline">
                <article>
                    <h3>1 · Prompt templating</h3>
                    <p>Authors lock key variables (addresses, hashes, timeframes) inside reusable templates to accelerate monthly refreshes.</p>
                </article>
                <article>
                    <h3>2 · Evidence capture</h3>
                    <p>Reference answers are harvested from official dashboards with timestamps and stored screenshots for auditing.</p>
                </article>
                <article>
                    <h3>3 · Dual evaluation</h3>
                    <p>Each task is run both as direct LLM calls and through the SmolAgent framework with identical tool permissions.</p>
                </article>
                <article>
                    <h3>4 · Scoring & QC</h3>
                    <p>Outputs are graded for factuality, synthesis, and prediction accuracy; two-person review resolves disputes and logs failure modes.</p>
                </article>
            </div>
            <div class="experiment-setup">
                <p><strong>Models:</strong> Grok-4 (Web & Fast), GPT-5, GPT-OSS-120B, Qwen3-Max, Gemini 2.5 Pro, DeepSeek R1, and others.</p>
                <p><strong>Metrics:</strong> Accuracy per quadrant, macro-category radar, difficulty split, and qualitative diagnostics.</p>
                <p><strong>Time window:</strong> 12–18 Oct, ensuring identical market conditions for every run.</p>
            </div>
        </section>

        <section class="section" id="failures">
            <div class="section-header">
                <h2>Observed Failure Modes</h2>
                <p class="section-subtitle">Section 5.4 highlights four recurring issues.</p>
            </div>
            <div class="card-grid">
                <article class="card">
                    <h3>Shallow search & source fidelity</h3>
                    <p>Agents quote outdated blog posts instead of navigating to official protocol dashboards for live TVL or liquidation data.</p>
                </article>
                <article class="card">
                    <h3>Stale information</h3>
                    <p>Cached snippets lag behind volatile metrics (e.g., reporting 14% liquidation share when the reference is 79%).</p>
                </article>
                <article class="card">
                    <h3>Integration errors</h3>
                    <p>Models retrieve correct numbers yet fail to compute spreads or rankings—concluding SOL outperformed BTC in Q1 2025 when the opposite is true.</p>
                </article>
                <article class="card">
                    <h3>Prediction hallucination</h3>
                    <p>Agents fabricate trending-token lists instead of grounding predictions in gmgn.ai or other mandated sources.</p>
                </article>
            </div>
        </section>

        <section class="section" id="figures">
            <div class="section-header">
                <h2>Figures from the Paper</h2>
                <p class="section-subtitle">Each PDF is rendered full-width so readers can inspect the benchmark visuals without leaving the page. Captions are sourced from the relevant sections of <code>main.tex</code>.</p>
            </div>
            <div class="figure-stack">
                <article class="figure-full">
                    <h3>Figure 1. Combined Evaluation Scores</h3>
                    <p>This overview contrasts direct LLM evaluation with the SmolAgent setup between 12–18 Oct, showing how agentic scaffolding reshuffles rankings.</p>
                    <iframe class="figure-embed" src="assets/pdfs/3_combined_eval_scores.pdf#toolbar=0&view=FitH" title="Combined Evaluation Scores"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/3_combined_eval_scores.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 2. LLM-only Leaderboard</h3>
                    <p>Direct prompts reveal Grok-4 (Web) clearly leading at 1.32 points, with GPT-5 trailing at 0.90 and a long tail of underperforming models.</p>
                    <iframe class="figure-embed" src="assets/pdfs/1_llm_eval_scores.pdf#toolbar=0&view=FitH" title="LLM Evaluation Scores"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/1_llm_eval_scores.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 3. SmolAgent Leaderboard</h3>
                    <p>When the same models run inside SmolAgent, Grok-4 still leads but Qwen3-Max and Gemini 2.5 Pro close the gap—evidence that planning and tool use matter.</p>
                    <iframe class="figure-embed" src="assets/pdfs/2_smolagent_eval_scores.pdf#toolbar=0&view=FitH" title="SmolAgent Evaluation Scores"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/2_smolagent_eval_scores.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 4. Task Distribution</h3>
                    <p>CryptoBench balances the four quadrants so no single capability can game the benchmark or inflate scores.</p>
                    <iframe class="figure-embed" src="assets/pdfs/A_task_distribution.pdf#toolbar=0&view=FitH" title="Task Distribution"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/A_task_distribution.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 5. Platform Distribution</h3>
                    <p>Agents must interact with 20+ live surfaces—from on-chain explorers to intent-based MEV dashboards—to answer correctly.</p>
                    <iframe class="figure-embed" src="assets/pdfs/B_platform_distribution.pdf#toolbar=0&view=FitH" title="Platform Distribution"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/B_platform_distribution.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 6. Performance by Difficulty</h3>
                    <p>All models degrade on complex tasks; Grok-4 drops roughly 10 percentage points while GPT-5 collapses by more than 25.</p>
                    <iframe class="figure-embed" src="assets/pdfs/C_performance_by_difficulty.pdf#toolbar=0&view=FitH" title="Performance by Difficulty"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/C_performance_by_difficulty.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 7. Retrieval vs. Prediction</h3>
                    <p>The retrieval–prediction imbalance is stark: GPT-5 reaches 41% on retrieval but only 6.25% on prediction.</p>
                    <iframe class="figure-embed" src="assets/pdfs/D_performance_by_task_family.pdf#toolbar=0&view=FitH" title="Performance by Task Family"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/D_performance_by_task_family.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 8. Macro-category Radar</h3>
                    <p>Top models perform best in general “Other” tasks, while DeFi analytics, DEX data, and derivatives remain challenging.</p>
                    <iframe class="figure-embed" src="assets/pdfs/E_performance_by_category_radar.pdf#toolbar=0&view=FitH" title="Performance by Category"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/E_performance_by_category_radar.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 9. Task Quadrants</h3>
                    <p>Simple retrieval is saturated, but complex prediction exposes the largest gaps, confirming the need for multi-hop reasoning.</p>
                    <iframe class="figure-embed" src="assets/pdfs/F_performance_by_task_quadrant.pdf#toolbar=0&view=FitH" title="Performance by Quadrant"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/F_performance_by_task_quadrant.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 10. Four-Quadrant Framework</h3>
                    <p>Illustrates the Simple/Complex × Retrieval/Prediction matrix that guides task authoring and score reporting.</p>
                    <iframe class="figure-embed" src="assets/pdfs/Figure1.pdf#toolbar=0&view=FitH" title="Quadrant Framework"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/Figure1.pdf" download>Download PDF</a></p>
                </article>
                <article class="figure-full">
                    <h3>Figure 11. Benchmark Pipeline</h3>
                    <p>Shows the end-to-end workflow: question authoring, data capture, agent execution, scoring, and diagnostics.</p>
                    <iframe class="figure-embed" src="assets/pdfs/Figure2.pdf#toolbar=0&view=FitH" title="Benchmark Pipeline"></iframe>
                    <p class="figure-meta"><a href="assets/pdfs/Figure2.pdf" download>Download PDF</a></p>
                </article>
            </div>
        </section>

        <section class="section" id="downloads">
            <div class="section-header">
                <h2>Download the CryptoBench Kit</h2>
                <p class="section-subtitle">Static site + TeX source = instant GitHub Pages deployment.</p>
            </div>
            <div class="download-actions">
                <a class="btn btn-primary" href="https://github.com/hsz0403/CryptoBench.github.io" target="_blank" rel="noopener">GitHub Repository</a>
                <a class="btn btn-secondary" href="#" aria-disabled="true">Paper PDF (soon)</a>
            </div>
        </section>
    </main>

    <footer>
        <p>© CryptoBench. Dynamic benchmark for expert-level evaluation of LLM agents in cryptocurrency.</p>
    </footer>
</body>
</html>
